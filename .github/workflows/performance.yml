name: Performance and Integration Testing

on:
  push:
    branches: [ master, main, develop ]
  pull_request:
    branches: [ master, main, develop ]
  schedule:
    # Run performance tests weekly on Sunday at 3 AM UTC
    - cron: '0 3 * * 0'
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: false
        type: choice
        options:
          - 'all'
          - 'unit'
          - 'integration'
          - 'performance'
        default: 'all'

env:
  PYTHON_VERSION: '3.11'

jobs:
  # Performance Benchmarks
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          enable-cache: true

      - name: Install dependencies
        run: |
          uv sync --dev
          uv run pip install pytest-benchmark memory-profiler psutil

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y xvfb libegl1-mesa libxkbcommon-x11-0

      - name: Create benchmark directory
        run: |
          mkdir -p benchmarks

      - name: Run performance benchmarks
        run: |
          xvfb-run -a uv run pytest tests/ -m "not slow" --benchmark-only --benchmark-json=benchmarks/benchmark-results.json --benchmark-sort=mean
        env:
          QT_QPA_PLATFORM: offscreen

      - name: Memory profiling
        run: |
          uv run python -m memory_profiler main.py --help > benchmarks/memory-profile.txt || true

      - name: Store benchmark results
        uses: benchmark-action/github-action-benchmark@v1
        if: github.event_name != 'pull_request'
        with:
          tool: 'pytest'
          output-file-path: benchmarks/benchmark-results.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          comment-on-alert: true
          alert-threshold: '150%'
          fail-on-alert: false

      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: benchmarks/

  # Load Testing
  load-test:
    name: Load Testing
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event.inputs.benchmark_type == 'all' || github.event.inputs.benchmark_type == 'performance'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v4

      - name: Install dependencies
        run: |
          uv sync --dev
          uv run pip install locust

      - name: Create load test script
        run: |
          cat > load_test.py << 'EOF'
          from locust import HttpUser, task, between
          import os
          
          class VidTaniumUser(HttpUser):
              wait_time = between(1, 3)
              
              def on_start(self):
                  """Setup for each user"""
                  pass
              
              @task(3)
              def test_basic_functionality(self):
                  """Test basic application functionality"""
                  # This would test actual endpoints if the app had a web interface
                  pass
              
              @task(1)
              def test_heavy_operation(self):
                  """Test resource-intensive operations"""
                  # This would test download/processing operations
                  pass
          EOF

      - name: Run load test (dry run)
        run: |
          echo "Load testing would be performed here with actual endpoints"
          echo "Current setup is for demonstration purposes"

  # Integration Tests with External Services
  integration-test:
    name: Integration Tests
    runs-on: ubuntu-latest
    services:
      # Add any required services here (e.g., databases, message queues)
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v4

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y xvfb libegl1-mesa libxkbcommon-x11-0 ffmpeg

      - name: Install dependencies
        run: |
          uv sync --dev

      - name: Install Playwright browsers
        run: |
          uv run playwright install --with-deps chromium

      - name: Wait for services
        run: |
          # Wait for Redis to be ready
          timeout 30 bash -c 'until redis-cli -h localhost -p 6379 ping; do sleep 1; done'

      - name: Run integration tests
        run: |
          xvfb-run -a uv run pytest tests/integration/ -v --tb=short --junit-xml=integration-results.xml
        env:
          QT_QPA_PLATFORM: offscreen
          REDIS_URL: redis://localhost:6379

      - name: Upload integration test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-test-results
          path: integration-results.xml

  # End-to-End Testing
  e2e-test:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    if: github.event_name != 'pull_request' || contains(github.event.pull_request.labels.*.name, 'e2e-test')
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v4

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y xvfb libegl1-mesa libxkbcommon-x11-0 ffmpeg

      - name: Install dependencies
        run: |
          uv sync --dev

      - name: Install Playwright browsers
        run: |
          uv run playwright install --with-deps chromium firefox

      - name: Create test data
        run: |
          mkdir -p test-data
          # Create sample test files if needed

      - name: Run E2E tests
        run: |
          xvfb-run -a uv run pytest tests/ -m "integration and not slow" -v --tb=short --junit-xml=e2e-results.xml
        env:
          QT_QPA_PLATFORM: offscreen

      - name: Upload E2E test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-test-results
          path: |
            e2e-results.xml
            test-data/

  # Performance Regression Detection
  performance-regression:
    name: Performance Regression Detection
    runs-on: ubuntu-latest
    needs: [benchmark]
    if: github.event_name == 'pull_request'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results
          path: current-benchmarks/

      - name: Checkout base branch
        run: |
          git fetch origin ${{ github.base_ref }}
          git checkout origin/${{ github.base_ref }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v4

      - name: Install dependencies
        run: |
          uv sync --dev
          uv run pip install pytest-benchmark

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y xvfb libegl1-mesa libxkbcommon-x11-0

      - name: Run baseline benchmarks
        run: |
          mkdir -p baseline-benchmarks
          xvfb-run -a uv run pytest tests/ -m "not slow" --benchmark-only --benchmark-json=baseline-benchmarks/benchmark-results.json --benchmark-sort=mean
        env:
          QT_QPA_PLATFORM: offscreen

      - name: Compare performance
        run: |
          uv run python -c "
          import json
          import sys
          
          # Load current and baseline results
          with open('current-benchmarks/benchmark-results.json') as f:
              current = json.load(f)
          
          with open('baseline-benchmarks/benchmark-results.json') as f:
              baseline = json.load(f)
          
          # Compare benchmarks
          regressions = []
          improvements = []
          
          current_benchmarks = {b['name']: b for b in current['benchmarks']}
          baseline_benchmarks = {b['name']: b for b in baseline['benchmarks']}
          
          for name, current_bench in current_benchmarks.items():
              if name in baseline_benchmarks:
                  baseline_bench = baseline_benchmarks[name]
                  current_mean = current_bench['stats']['mean']
                  baseline_mean = baseline_bench['stats']['mean']
                  
                  change_percent = ((current_mean - baseline_mean) / baseline_mean) * 100
                  
                  if change_percent > 10:  # 10% slower is a regression
                      regressions.append((name, change_percent))
                  elif change_percent < -10:  # 10% faster is an improvement
                      improvements.append((name, abs(change_percent)))
          
          # Output results
          if regressions:
              print('Performance Regressions Detected:')
              for name, change in regressions:
                  print(f'  - {name}: {change:.1f}% slower')
              sys.exit(1)
          
          if improvements:
              print('Performance Improvements:')
              for name, change in improvements:
                  print(f'  - {name}: {change:.1f}% faster')
          
          print('No significant performance regressions detected.')
          "

      - name: Upload comparison results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-comparison
          path: |
            current-benchmarks/
            baseline-benchmarks/

  # Test Summary
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [benchmark, integration-test, e2e-test]
    if: always()
    steps:
      - name: Create test summary
        run: |
          echo "# Performance and Integration Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Test Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Benchmarks**: ${{ needs.benchmark.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Integration Tests**: ${{ needs.integration-test.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- **E2E Tests**: ${{ needs.e2e-test.result }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [[ "${{ needs.benchmark.result }}" == "failure" || 
                "${{ needs.integration-test.result }}" == "failure" || 
                "${{ needs.e2e-test.result }}" == "failure" ]]; then
            echo "⚠️ Some tests failed. Please review the logs for details." >> $GITHUB_STEP_SUMMARY
            exit 1
          else
            echo "✅ All tests passed successfully!" >> $GITHUB_STEP_SUMMARY
          fi
